---
title: 'Análisis de Twitter en periodos de emergencia: Sismo 19 del septiembre del
  2017, CDMX'
author: "Carolina Acosta, Eduardo Moreno & Elena Villalobos"
output:
  html_document:
    theme: lumen
    hightlight: kate
always_allow_html: true
---
***
***

## 1. Introducción

Las redes sociales en periodos de emergencia ha incrementado su importancia por la rapidez de la comunicación y el alcance que tienen. Una de las redes sociales que más presencia tiene en este ámbito, es *Twitter*, debido a que la mayoría de entidades gubernamentales tienen una cuenta oficial por la cuál realizan comunicados. 

El análisis del comportamiento en redes sociales como *Twitter* puede realizarse de varias maneras. Una, es estudiando las relaciones entre los diferentes agentes y ver cuáles son los usuarios más relevantes o únicos en dichas interacciones. Otra forma de analizar este comportamiento, es analizando el contenido del mismo texto para ver el tipo de mensajes que la gente está transmitiendo, si son similares o difiren mucho entre ellos. 

Estos enfoques nos pueden ayudar a entender de manera amplia, el comportamiento de las personas en dicha red social, sobre todo en periodos de emergencia cuando es crucial estar atentos a información oficial y verídica.

### 1.1. Objetivo

##### El objetivo del presente trabajo, es estudiar el comportamiento de usuarios a través de la red social *Twitter*, en un evento específico como sismo del 19 de septiembre del 2017 en la Ciudad de México. Específicamente, se utilizará Locality Sensitive Hashing (LSH) para agrupar colecciones de los *tweets*  que tengan alta similitud y se realizará un análisis de redes para identificar usuarios clave en este tipo de eventos. 

**El código asociado al presente trabajo lo encuentras en el siguiente [repositorio](https://github.com/caroacostatovany/analisis-de-tweets).**

**Palabras clave: Locality Sensitive Hashing, Análisis de redes, Minería de texto, Twitter.**

### 1.2. Metodología

Para tener acceso a los *tweets* se realizó una descarga desde la *API*. Para realizar dicha descarga, se tuvo que completar una aplicación a [Developer Twitter](https://developer.twitter.com/en) como investigador académico explicando las intenciones del estudio. Posteriormente, se obtuvo el acceso a través tokens y llaves secretas proporcionadas por *Twitter*.

Se descargaron alrededor de 80,000 *tweets* de los cuáles se obtuvimos **65,489 tweets con clave única, incluyendo retweets**. Los datos extraídos son del 11 de septiembre de 2017 al 31 de diciembre de 2017. Se utilizaron como referencia de palabras `sismo cdmx`, `#MexicoNosNecesita`, `ayuda sismo` y `19s`.

El contenido de la base tiene las siguientes columnas:

- `created_at`: la fecha en la que se publicó el *tweet*.
- `id`: ID único del *tweet*.
- `author_id`: ID del usuario del *tweet*.
- `reply_to_user_id`: diccionario del tipo de *tweet* (si fue *retweet*) y del ID del usuario a quien hizo *retweet*.
- `entities`: diccionario de nombres de usuario, *hashtags* y URLs mencionados en el *tweet*.
- `text`: texto del *tweet*.
- `username`: nombre de usuario que publicó el *tweet*.

## 2. Limpieza y análisis de texto

Antes de iniciar cualquier tipo de análisis decidimos limpiar el texto del *tweet* y agregar dos columnas para que el análisis de redes fuera más sencillo de realizar. Las columnas agregadas fueron: nombre de usuario a quien se le hizo *retweet* y nombres de usuarios mencionados en el *tweet*. En las tareas de limpieza y agregado de columnas se utilizaron expresiones regulares.

Iniciamos con la adición de columnas:

- Nombre de usuario del *retweet*: Se obtuvo a partir del *tweet* extrayendo el usuario mencionado cuando existía "RT @" en el mismo *tweet*.
- Nombres de usuarios mencionados: Se obtuvieron todas las menciones en el *tweet* y se remueve el nombre de usuario del *retweet* si existe.

Posteriormente, realizamos la limpieza del texto del *tweet* donde las tareas fueron las siguientes:

- Remover:
  - Acentos.
  - Caracteres especiales como "’"?¡!*,.():;-_¿".
  - La secuencia "RT" haciendo referencia al retweet.
  - Menciones de usuarios.
  - URLs especificadas como www. o https:
  - Espacios extras.
  
## 3. Minhashing - Locality Sensitive Hashing 

El objetivo de utilizar Locality Sensitive Hashing (LSH) en este proyecto, es agrupar colecciones de los *tweets* obtenidos que tienen alta similitud. Construimos el LSH basándonos en las firmas de minhash, y así asignar el documento en una cubeta dependiendo de ésta.


```{r library, include=FALSE}
library(tidyverse)
library(ggplot2) 

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, 
                      fig.align = 'center', 
                      #fig.width = 5, fig.height=3, 
                      cache = TRUE)
theme_set(theme_minimal(base_size = 14))
```

```{r functions, include=FALSE}
sim_jaccard <- function(a, b){
    length(intersect(a, b)) / length(union(a, b))
}

calcular_tejas <- function(x, k = 2){
  tokenizers::tokenize_character_shingles(x, n = k, lowercase = FALSE,
    simplify = TRUE, strip_non_alpha = FALSE)
}

generar_hash <- function(){
    r <- as.integer(stats::runif(1, 1, 2147483647))
    funcion_hash <- function(shingles){
        digest::digest2int(shingles, seed =r) 
    }
    funcion_hash
}

crear_tejas_str <- function(textos, k = 4){
    num_docs <- length(textos)
    tejas <- calcular_tejas(textos, k = k)
    tejas_df <- tibble(doc_id = 1:num_docs, tejas = tejas)
    tejas_df
}

calcular_firmas_doc <- function(tejas_df, hash_funs){
    # Calcula firmas por documento
    num_docs <- nrow(tejas_df)
    num_hashes <- length(hash_funs)
    tejas <- tejas_df$tejas
    firmas <- vector("list", num_docs)
    # este se puede paralelizar facilmente:
    for(i in 1:num_docs){
        firmas[[i]] <- map_dbl(hash_funs, ~ min(.x(tejas[[i]])))
    }
    tibble(doc_id = 1:num_docs, firma = firmas)
}

separar_cubetas_fun <- function(particion){
    function(firma){
        map_chr(particion, function(x){
            prefijo <- paste0(x, collapse = '')
            cubeta <- paste(firma[x], collapse = "/")
            paste(c(prefijo, cubeta), collapse = '|')
        })
    }
}

extraer_pares <- function(cubetas_df, cubeta, docs, textos = NULL, names = NULL){
   enq_cubeta <- enquo(cubeta)
   enq_docs <- enquo(docs)
   pares <- cubetas_df %>% 
    group_by(!!enq_cubeta) %>% 
    mutate(pares = map(!!enq_docs, ~ combn(sort(.x), 2, simplify = FALSE))) %>%
    select(!!enq_cubeta, pares) %>% unnest_legacy %>% 
    mutate(a = map_int(pares, 1)) %>% 
    mutate(b = map_int(pares, 2)) %>% 
    select(-pares) %>% ungroup %>% select(-!!enq_cubeta) %>% 
    unique #quitar pares repetidos
   if(!is.null(textos)){
       pares <- pares %>% mutate(usuario_a = names[a], usuario_b = names[b], texto_a = textos[a], texto_b = textos[b])
   }
   pares %>% ungroup 
}
#tweets <- read_csv('../data/tweets_limpios_2021_05_15.csv')
tweets <- read.csv('/Volumes/MemoriaEle/HeavyData/tweets_sismo/tweets_limpios_2021_05_12.csv')
```

Empezemos el análisis de *tweets* incluyendo *retweets*:

```{r message=FALSE, warning=FALSE, eval=FALSE}
tweets <- read_csv('../data/tweets_limpios_2021_05_15.csv')
```

```{r lectura, message=FALSE, warning=FALSE}
tweets_texto <- tweets$texto_limpio
```

Creamos el hash (la firma) a partir de las tejas, de las cuales se ha decidido utilizar 6 caracteres para crear las tejas y así mapear cada teja a un entero.

```{r}
set.seed(20210512)
hash_f <- map(1:12, ~ generar_hash())
tejas_tbl <- crear_tejas_str(tweets_texto, k = 6)
firmas_tw <- calcular_firmas_doc(tejas_tbl, hash_f)
```

Una vez obtenida la firma de cada documento y creamos una cubeta para cada firma diferente, las firmas que se encuentran en la misma cubeta son documentos candidatos a ser similares. Se ha decidido capturar pares de documentos con similitud más baja y así agrupar textos con algún grupo de **7** minhashes iguales.

```{r}
particion <- split(1:12, ceiling(1:12 / 7))
sep_cubetas <- separar_cubetas_fun(particion) 
#sep_cubetas(firmas_tw$firma[[1]])
cubetas_tbl <- firmas_tw %>%
    mutate(cubeta = map(firma, sep_cubetas)) %>%
    unnest_legacy(cubeta) %>% 
    group_by(cubeta) %>% 
    summarise(docs = list(doc_id), n = length(doc_id)) %>%
    arrange(desc(n))

# % de cubetas con documentos únicos
nrow(cubetas_tbl %>% filter(., n==1)) / nrow(cubetas_tbl) * 100
cubetas_tbl %>% arrange(desc(n)) %>% head(10)
```
En la tabla anterior podemos observar que obtuvimos 25,957 cubetas de las cuales la más grande contiene 2,333 documentos, esto nos indica que tenemos muchos *tweets* parecidos o iguales. Si hacemos un análisis básico , véase la siguiente gráfica, podemos obtener que aproximadamente el 60% de nuestras cubetas contienen un único documentos, por lo que podemos suponer que el 60% de los usuarios hablan de tópicos diferentes acerca del sismo del 2017 y el 40% restante son *retweets* o copias de los *tweets*.

```{r}
# filtramos las cubetas que tienen menos de 10 documentos para ejercicio visual de la gráfica
cubetas_tbl_2 <- filter(cubetas_tbl, n>10)

ggplot() + 
  geom_line(data=cubetas_tbl_2, aes(x=as.numeric(row.names(cubetas_tbl_2)), y=n), color="#10D6C1") + 
  labs(title="No. de documentos por cubeta", y="no. de documentos", x="id cubeta")
```
Como vimos que aproximadamente el 40% de nuestros datos son *retweets* y no pudimos obtener mucha información de este análisis; por lo que se ha realizado el mismo proceso pero removiendo los *tweets* duplicados; de 65,489 *tweets* nos quedaron 15,101.

```{r}
tw_hashes <- digest::digest2int(tweets_texto)
tw_dedup <- tibble(tweet = tweets_texto, hash = tw_hashes) %>% 
  group_by(hash) %>% 
  summarise(tweet = tweet[1], .groups = "drop") %>%  
  mutate(longitud = nchar(tweet)) %>% 
  filter(longitud >= 5) %>% # Quitamos los tweets con 5 caracteres
  pull(tweet)

tw_dedup_2 <- tibble(tweet = tweets_texto, usuario=tweets$username, hash = tw_hashes) %>% 
  group_by(hash) %>% 
  summarise(tweet = tweet[1], usuario=usuario[1], .groups = "drop") %>%  
  mutate(longitud = nchar(tweet)) %>% 
  filter(longitud >= 5) %>% 
  pull(tweet, usuario)

length(tweets_texto)
length(tw_dedup)
```
Repetimos el mismo proceso realizado anteriormente para crear los hashes a partir de las tejas y separar por cubetas.

```{r}
set.seed(20210512)
hash_f <- map(1:12, ~ generar_hash())
tejas_tbl <- crear_tejas_str(tw_dedup, k = 6)
firmas_tw <- calcular_firmas_doc(tejas_tbl, hash_f)
particion <- split(1:12, ceiling(1:12 / 7))
sep_cubetas <- separar_cubetas_fun(particion) 
#sep_cubetas(firmas_tw$firma[[1]])
cubetas_tbl <- firmas_tw %>%
    mutate(cubeta = map(firma, sep_cubetas)) %>%
    unnest_legacy(cubeta) %>% 
    group_by(cubeta) %>% 
    summarise(docs = list(doc_id), n = length(doc_id)) %>%
    arrange(desc(n))

# % de cubetas con documentos únicos
nrow(cubetas_tbl %>% filter(., n==1)) / nrow(cubetas_tbl) * 100
cubetas_tbl %>% arrange(desc(n)) %>% head(10)
```
En la tabla anterior, podemos observar que mantuvimos las 25,957 cubetas pero el número de documentos por cubeta bajó y ahora el máximo de documentos en una cubeta es de 25 documentos. Y esta vez obtenemos un aproximado del 90% de cubetas con un documento único.

```{r}
# filtramos las cubetas que tienen menos de 2 documentos para ejercicio visual de la gráfica
cubetas_tbl_2 <- filter(cubetas_tbl, n>2)

ggplot() + 
  geom_line(data=cubetas_tbl_2, aes(x=as.numeric(row.names(cubetas_tbl_2)), y=n), color="#10D6C1") + 
  labs(title="No. de documentos por cubeta", y="no. de documentos", x="id cubeta") 
```

Si evaluamos una cubeta con varios documentos, por ejemplo la segunda cubeta, podemos observar que los textos a pesar de que tienen diferentes cifras el contenido es muy parecido y hace referencia a albergues donde pernoctaron las personas tras el sismo.
```{r}
DT::datatable(cubetas_tbl$docs[[2]] %>% data.frame(tweet=tw_dedup[.],usuario=attr(tw_dedup_2[.],'names')))
```

Una vez obteniendo las cubetas podemos encontrar eficientemente los pares de similitud alta; ya que sólo se hará la evualación en los pares de documentos dentro de cada cubeta y se podrán filtrar los documentos en los que tengan menor a 30% de similitud.

```{r}
# Filtramos las cubetas en donde se pueden hacer pares
cubetas_nu_tbl <- filter(cubetas_tbl, n > 2)

# Agregar extracción de usuario a y usuario b
pares_candidatos <- extraer_pares(cubetas_nu_tbl, cubeta, docs, textos = tw_dedup, names=attr(tw_dedup_2, 'names')) %>% 
                  arrange(texto_a)

pares_scores <- pares_candidatos %>% 
  mutate(score = map2_dbl(texto_a, texto_b,
  ~ sim_jaccard(calcular_tejas(.x, 5), calcular_tejas(.y, 5)))) %>% arrange(desc(score))  %>% filter(score > 0.3)
DT::datatable(pares_scores)
```

Dado que en este análisis encontramos que muchos usuarios hacen *retweet* más que tener texto único, se optó por realizar un análisis con redes que en breve explicaremos.


## 4. Análisis de Redes

Para hacer el análisis de redes del presente trabajo, utilizamos:

- Los usuarios propietarios del *tweet*.
- Los usuarios que fueron mencionados en el *tweet*.
- Los usuarios que fueron retuiteados. 

Es decir, tenemos dos redes dirigidas, una que va del usuario propietario al usuario que fue mencionado por este último, y otra que va del usuario propietario al usuario que este mismo retuitea. Para estos análisis utilizamos las siguientes paqueterías, que están especializadas en este tipo de análisis. 

```{r, warning=FALSE}
library(tidygraph)
library(ggraph)
library(igraphdata)
```

```{r, include=FALSE, warning=FALSE}
library(viridis)
library(tidyverse)
library(patchwork)
# un poco de limpieza
usuarios <- tibble(tweets$username, tweets$usuarios_mencionados)

# hacemos una separación de todos los usuarios mencionados
# que están en un solo renglón
s <- strsplit(tweets$usuarios_mencionados, split =',')
usuarios <- tibble(u_propietario = 
                     rep(tweets$username, sapply(s, length)),
                   u_mencionado = unlist(s))

# quitamos espacios
usuarios$u_mencionado <- str_replace_all(usuarios$u_mencionado, " ", "")
usuarios$u_mencionado <- str_replace_all(usuarios$u_mencionado, ":", "")
```

#### Red dirigida de menciones

Para esta red contemplamos: 

- Nodos: 12,011
- Aristas: 25,060

que se conforma con las conexiones entre los usuarios propietarios y mencionados. Esta base esta estructurada de tal manera que si un mismo usuario propietario mencionó a varios usuarios, el primero se repite varias veces para mostrar la conexión de ese usuarios con otros más. Como se muestra en el siguiente objeto, que `loreCM505` mencionó en un mismo *tweet* a tres usuarios diferentes, pero aquí corresponde a un renglón diferente cada mención. 

```{r}
usuarios
```

Para formar la red utilizamos el objeto `as_tbl_graph()`, que identifica los nodos y los vértices:

```{r}
users_nodes_edges <- usuarios %>% as_tbl_graph()
users_nodes_edges
```

En el siguiente código creamos un objeto que identifica las conexiones y nos muestra su frecuencia. 

```{r}
vertices_users <- users_nodes_edges %>% 
  activate(edges) %>% 
  select(to, from) %>% 
  as_tibble()

vertices_agregados_users <- vertices_users %>% 
  group_by(to, from) %>% 
  summarise(freq_vert = n(), 
            .groups='drop') %>% 
    arrange(desc(freq_vert))
```

Con el objeto anterior, formamos los siguientes histrogramas. El de lado izquierdo nos muestra que tenemos muchas conexiones muy débiles donde la frecuencia de la arista es de 1, 2 o 3. El gráfico de la derecha, está filtrado con aristas mayores a 5. En este se puede apreciar que seguimos teniendo muchas conexiones débiles pero no tantas como si filtramos para conexiones arriba de 10. 

```{r, echo=FALSE, fig.dim=c(10,4)}
ho <- vertices_agregados_users %>% 
  arrange(desc(freq_vert)) %>% 
  #filter(freq_vert>5) %>% 
  ggplot(aes(x=freq_vert)) +
  geom_histogram(bins = 50,col='blue',
                 fill='blue',alpha=0.7)+
  xlab('Frecuencia de conexiones')+
  ylab('')
hi <- vertices_agregados_users %>% 
  arrange(desc(freq_vert)) %>% 
  filter(freq_vert>5) %>% 
  ggplot(aes(x=freq_vert)) +
  geom_histogram(bins = 50,col='blue',
                 fill='blue',alpha=0.7)+
  xlab('Frecuencia de conexiones > 5')+
  ylab('')

ho+hi

```

Con el siguiente objeto obtenemos los nodos y volvemos a armar la red con frecuencia de conexión arriba de 10. 

```{r}
nodos_users <- users_nodes_edges %>% 
  activate(nodes) %>% 
  as_tibble() 
nodos_users

users_nodes_edges_2 <- tbl_graph(
  nodes = nodos_users, 
  edges = vertices_agregados_users) 

corte_freq_vert <- 10
users_grandes <- users_nodes_edges_2 %>% 
  activate(edges) %>% 
  filter(freq_vert > corte_freq_vert) %>% 
  activate(nodes) %>% 
  filter(!node_is_isolated())
```


Y obtenemos la siguiente figura, que es una representación de la red que muestra las conexiones entre los nodos. En esta se observa una agrupación principal alrededor del usuario `DesSocial_CDMX`, que pareciera ser el nodo que tiene más **grado** pues tiene muchas aristas conectadas hacia él. Esta cuenta, correspondía a la Secretaría de Desarrollo Social de la Ciudad de México de ese periodo. También, se observan algunas agrupaciones alejadas y no conectadas a la agrupación principal, como en las que el `TecdeMonterrey`, `lasillarota` y `PGJDF_CDMX` son parte. Cercana a esta también se observan agrupaciones de 3 o 4 nodos. 

```{r, echo=FALSE, fig.dim=c(8,7)}
users_grandes %>% 
  activate(nodes) %>% 
  ggraph(layout = 'fr') + 
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 alpha = 0.5, colour="gray") + 
  geom_node_point(alpha=0.5,colour='turquoise3',size=3) +
  geom_node_text(aes(label=name), repel = TRUE, size=2)
```

Ahora, para seguir con el análisis de la red obtendremos la medida de intermediación con el siguiente código:

```{r}
# Componentes
compo_users <-  users_grandes %>% 
  activate(nodes) %>% 
  mutate(componente = group_components())

compo_users %>% 
  as_tibble %>% 
  group_by(componente) %>% 
  tally()

# filtramos por componente conexa más grande
usi <- users_grandes %>% 
  activate(nodes) %>% 
  mutate(componente = group_components()) %>% 
  filter(componente == 1)

# Calculamos la intermediación
usi <- usi %>% activate(nodes) %>% 
  mutate(intermediacion = centrality_betweenness())
```


La siguiente figura presenta la red tomando en cuenta la medida de **intermediación**, que recordemos que nos proporciona una medida indicando qué tan único o importante es un nodo para conectar con otros nodos en la red. Para este caso resalta mucho el usuario de `ManceraMiguelMX` y `GobCDMX`, que tienen una intermediación mucho más grande que incluso `DesSocial_CDMX`. Esto es debido a que tienen contienen conexiones únicas y en el caso de `ManceraMiguelMX` parece haber una conexión especial con cuentas oficiales como lo son el `C5_CDMX` y cuentas de noticieros oficiales. 

```{r, echo=FALSE, fig.dim=c(8,7)}
# Graficamos
usi %>% 
  activate(nodes) %>% 
  ggraph(layout = 'fr') +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 alpha = 0.5, colour="gray") + 
  geom_node_point(aes(size = intermediacion),
                  colour='orange') +
  geom_node_text(aes(label=name),repel = TRUE,  size=2)+
  theme_graph(base_family = "sans")
```

Ahora obtendremos la medida de centralidad de eigenvector o también conocida como **pagerank**. Dicha medida es una especie de promedio de la centralidad de sus vecinos. De manera más específica, esta medida considera que la importancia de un nodo está dado por la suma normalizada de las importancia de sus vecinos. De esta forma, es importante estar cercano a nodos importantes (como en cercanía), pero también cuenta conectarse a muchos nodos (como en grado). 

```{r}
usi <- usi %>%
  activate(nodes) %>% 
  mutate(central_eigen = centrality_eigen())
```

En el siguiente gráfico observamos la medida de centralidad de eigenvector tanto en el tamaño y el color del punto. Lo que observamos es que efectivamente importa estar cercano a nodos importantes y a su vez conectarse a muchos, que el usuario más relevante para nuestro conjunto de datos en menciones es `DesSocial_CDMX`, pues tiene el valor más alto, mientras que los otros usuarios tienen esta medida en menos de 0.25. 

```{r, echo=FALSE, fig.dim=c(8,6)}
usi %>%
  activate(nodes) %>% 
  ggraph(layout = 'graphopt', spring.constant = 0.25, charge = 0.05, niter = 300) + 
  geom_edge_link2(arrow = arrow(length = unit(2, 'mm')), alpha = 0.2, colour="gray") + 
  geom_node_point(aes(size = central_eigen, colour=central_eigen)) +
  geom_node_text(aes(label=name),  repel = TRUE,size=2)+
  theme_graph() +
  scale_color_viridis()
```


#### Red dirigida de *retweets*

Consideramos importante separar los *retweets* de las menciones, porque es un proceso diferente en *Twitter*, a continuación presentaremos sólo los gráficos correspondientes, pues el procesamiento para el análisis fue muy similar al ya presentado. 

- Nodos: 26,473.
- Aristas: 46,991.

```{r, include=FALSE}
usuarios <- tibble(tweets$username, tweets$retweet_user)
# hacemos una separación de todos los usuarios mencionados
# que están en un solo renglón
s <- strsplit(tweets$retweet_user, split =',')
usuarios <- tibble(u_propietario = 
                     rep(tweets$username, sapply(s, length)),
                   u_retuiteado = unlist(s))

# quitamos espacios
usuarios$u_retuiteado <- str_replace_all(usuarios$u_retuiteado, " ", "")
usuarios$u_retuiteado <- str_replace_all(usuarios$u_retuiteado, ":", "")
# tbl_graph identifica los nodos y los vertices
users_nodes_edges <- usuarios %>% as_tbl_graph()
```

Observamos la red construida:

```{r}
users_nodes_edges
```


```{r, include=FALSE}
# obtenemos los vertices
vertices_users <- users_nodes_edges %>% 
  activate(edges) %>% 
  select(to, from) %>% 
  as_tibble()

# agregamos los vertices por frecuencia
vertices_agregados_users <- vertices_users %>% 
  group_by(to, from) %>% 
  summarise(freq_vert = n(),
            .groups='drop')
```

El siguiente gráfico nos muestra que tenemos conexiones más fuertes en el caso de los retuits, por lo que utilizaremos un criterio de conexiones mayores a 15. 

```{r, echo=FALSE, fig.dim=c(5,4)}
vertices_agregados_users %>% 
  arrange(desc(freq_vert)) %>% 
  filter(freq_vert>5) %>% 
  ggplot(aes(x=freq_vert)) +
  geom_histogram(bins = 50,col='blue',
                 fill='blue',alpha=0.7)+
  xlab('Frecuencia de conexiones > 5')+
  ylab('')
```

```{r, include=FALSE}
# obtenemos solo los nodos 
nodos_users <- users_nodes_edges %>% 
  activate(nodes) %>% 
  as_tibble() 
nodos_users

# volvemos a armar la red
users_nodes_edges_2 <- tbl_graph(
  nodes = nodos_users, 
  edges = vertices_agregados_users) 
users_nodes_edges_2

# filtramos para los que tienen más frecuencia
corte_freq_vert <- 15
users_grandes <- users_nodes_edges_2 %>% 
  activate(edges) %>% 
  filter(freq_vert > corte_freq_vert) %>% 
  activate(nodes) %>% 
  filter(!node_is_isolated())
```


La siguiente figura es una representación de la red que muestra las conexiones entre los nodos. Se puede observar más agrupaciones entre los nodos, una de estas agrupaciones parece centrarse alrededor de `ManceraMiguelCDMX` y algunas cuentas oficiales del mismo gobierno. También, observamos otra agrupación con el grupo `Milenio` y el retuiteo de cuentas similares. Alrededor, de estos grupos hay muchos pares de nodos conectados entre sí, pero no conectados con otros usuarios. 

```{r, echo=FALSE, fig.dim=c(8,7)}
users_grandes %>% 
  activate(nodes) %>% 
  ggraph(layout = 'fr') + 
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 alpha = 0.5, colour="gray") + 
  geom_node_point(alpha=0.5,colour='turquoise3',size=3) +
  geom_node_text(aes(label=name), repel = TRUE, size=2)+
  theme_graph(base_family = "sans")
```

Para analizar con más detalle esta red, observamos la siguiente figura que es con la medida de **intermedicación**, que parece cambiar completamente la estructura de la red destacando `GobCDMX` y `seduviCDMX` que son los nodos con más intermediación de esta red. La cuenta de `ManceraMiguelCDMX` parece mantenerse en el centro de la red con muchas conexiones, pero no tiene tanta intermediación como los usuarios ya mencionados. 

```{r, include=FALSE}
# Componentes
compo_users <-  users_grandes %>% 
  activate(nodes) %>% 
  mutate(componente = group_components())

compo_users %>% 
  as_tibble %>% 
  group_by(componente) %>% 
  tally()

# filtramos por componente conexa más grande
usi <- users_grandes %>% 
  activate(nodes) %>% 
  mutate(componente = group_components()) %>% 
  filter(componente == 1)

# Calculamos la intermediación
usi <- usi %>% activate(nodes) %>% 
  mutate(intermediacion = centrality_betweenness())
```


```{r, echo=FALSE, fig.dim=c(8,7)}
# Graficamos
usi %>% 
  activate(nodes) %>% 
  ggraph(layout = 'fr') +
  geom_edge_link(arrow = arrow(length = unit(1, 'mm')), 
                 alpha = 0.3, colour="gray") + 
  geom_node_point(aes(size = intermediacion),
                  colour='orange') +
  geom_node_text(aes(label=name),repel = TRUE,  size=2)+
  theme_graph(base_family = "sans")
```

Por último, observemos el gráfico de la medida de centralidad de eigenvector. 

```{r, include=FALSE}
usi <- usi %>%
  activate(nodes) %>% 
  mutate(central_eigen = centrality_eigen())
```

```{r, echo=FALSE, fig.dim=c(8,6)}
usi %>%
  activate(nodes) %>% 
  ggraph(layout = 'graphopt', spring.constant = 0.25, charge = 0.05, niter = 300) + 
  geom_edge_link2(arrow = arrow(length = unit(2, 'mm')), alpha = 0.2, colour="gray") + 
  geom_node_point(aes(size = central_eigen, colour=central_eigen)) +
  geom_node_text(aes(label=name),  repel = TRUE,size=2)+
  theme_graph() +
  scale_color_viridis()
```

#### Comparación de redes

En este análisis se observó que si parece haber un proceso un poco diferente a la hora de realizar retuits o hacer menciones de otros usuarios. Por ejemplo, a pesar de que `ManceraMiguelCDMX` destacó en ambas redes debido a que fue el jefe de gobierno de la CDMX, durante el sismo, obtuvo mayor relevancia en la red de retuits que de menciones.

También, observamos que cambian mucho las redes dependiendo de la medida que utilices, si bien el número de conexiones parecen ser importantes, también lo es ser único cuando te conectas con otros usuarios. 

## 5. Conclusiones

Dado un fenómeno catastrófico como el terremoto analizado, el cual causó pérdidas humanas y monetarias, lo que más importa es que los individuos estén informados con el objetivo de prevenir mayores pérdidas y ayudar a los necesitados. Por medio de Twitter puede lograrse dicho objetivo y es aquí donde la métrica de intermediación toma relevante importancia, pues dicha métrica indica cuán importante es un usuario de Twitter para informar a otros. Complementando con la métrica de pagerank, podemos obtener aquellos usuarios que tienen relevancia dado que permiten la conexión con muchos demás usuarios, así como usuarios de mayor o similar relevancia que los primeros.

Lo anterior parece confirmar, por lo menos para esta muestra, que los usuarios efectivamente consideran las cuentas oficiales para compartir información relevante. Además, de que como observamos en el análisis de LSH se comparte contenido para pedir ayuda o comunicar información relevante. 

Un reto importante en este estudio fue conseguir más *tweets*, que por el proceso mismo de descarga en el cual utilizamos palabras clave, pudiéramos tener un sesgo en la muestra. Descargar *tweets* con otras palabras clave, analizarlos y repetir este proceso de manera iterativa, pudiera darnos un mejor indicio del comportamiento general de los usuarios en situaciones de emergencia. 

#### Referencias:

- Curso de Métodos Analíticos de Fepile González,  [notas](https://metodos-analiticos-2021.netlify.app/). 

***

***

17/05/2021 